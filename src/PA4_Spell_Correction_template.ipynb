{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i3m9JjeM5U5"
      },
      "source": [
        "# **Programming Assessment \\#4**\n",
        "\n",
        "Names: \\<please supply your names\\>\n",
        "\n",
        "More information on the assessment is found in our Canvas course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxtmCAZwNoeU"
      },
      "source": [
        "# **Load Data**\n",
        "\n",
        "*While you don't have to separate your code into blocks, it might be easier if you separated loading your data from actually implementation of your code. Consider placing all loading of data into the code block below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       count         P\n",
            "error                 \n",
            "e|i      917  0.023469\n",
            "a|e      856  0.021908\n",
            "i|e      771  0.019732\n",
            "e|a      749  0.019169\n",
            "a|i      559  0.014307\n",
            "...      ...       ...\n",
            " |c        1  0.000026\n",
            " |a        1  0.000026\n",
            " |'        1  0.000026\n",
            " w|        1  0.000026\n",
            " t|t       1  0.000026\n",
            "\n",
            "[1587 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "# The code is defining a function called `make_p` that takes a DataFrame `df` as input.\n",
        "# lang must contain a column 'count' that contains number types\n",
        "#load error data\n",
        "import pandas as pd\n",
        "def make_p(lang):\n",
        "    total = lang['count'].sum()\n",
        "    inv = 1/total\n",
        "    df['P'] = err['count']*inv\n",
        "    return df\n",
        "x=''\n",
        "\n",
        "with open(\"count_1edit.txt\", \"r\") as f:\n",
        "    x = f.read().splitlines()\n",
        "gen = (dat.split('\\t') for dat in x)\n",
        "data = [(rec[0],int(rec[1]))for rec in gen]\n",
        "err = pd.DataFrame.from_records(data, columns=['error', 'count'])\n",
        "err = make_p(err)\n",
        "err = err.set_index(['error'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "CbvxU2oTM4IV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading gutenburg: Package 'gutenburg' not found in\n",
            "[nltk_data]     index\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#loading corpus files\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "nltk.download('gutenburg')\n",
        "# nltk.corpus.gutenberg.fileids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting all documents from NLTK's Project Gutenberg Collection...\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "import string\n",
        "print(\"Extracting all documents from NLTK's Project Gutenberg Collection...\")\n",
        "all_words = Counter()\n",
        "for filename in nltk.corpus.gutenberg.fileids():\n",
        "  words = [word.lower() for word in nltk.corpus.gutenberg.words(filename) if word not in string.punctuation]\n",
        "  all_words.update(words)\n",
        "lang = pd.DataFrame.from_dict(all_words, orient='index').reset_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "lang = lang.rename(columns={'index':'word', 0:'count'})\n",
        "lang = make_p(lang)\n",
        "lang = lang.set_index(['word'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8YCZLi-N0uR"
      },
      "source": [
        "# **Noisy Channel Model Implementation**\n",
        "\n",
        "*Again, you don't have to follow this directly, but consider placing your implementation of the model in the code block below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def find_err(str_1, str_2):\n",
        "    rows = len(str_1) + 1\n",
        "    cols = len(str_2) + 1\n",
        "    dMatrix = np.zeros((rows, cols), dtype=int)\n",
        "    dMatrix[0]= [*range(0, cols)]\n",
        "    for i in range(1, rows):\n",
        "        dMatrix[i][0] = i\n",
        "            \n",
        "        \n",
        "    for col in range(1, cols):\n",
        "        for row in range(1, rows):\n",
        "            if str_1[row - 1] == str_2[col - 1]:\n",
        "               cost = 0\n",
        "            else:\n",
        "               cost = 1\n",
        "            if str_1[row - 1] == str_2[col] and str_1[row] == str_2[col-1]:\n",
        "                # transpose\n",
        "                pass\n",
        "            dMatrix[row][col] = min(dMatrix[row - 1][col] + 1, dMatrix[row][col - 1] + 1, dMatrix[row-1][col-1] + cost)\n",
        "\n",
        "    \n",
        "    return dMatrix[row][col]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shawn's trash\n",
        "# from collections import Counter\n",
        "# import numpy as np\n",
        "# from re import finditer,compile\n",
        "# from editdistpy import damerau_osa as ld\n",
        "# import pandas as pd\n",
        "# import nltk\n",
        "# from nltk import FreqDist as f_dist\n",
        "# from nltk.corpus import gutenberg as cor_g\n",
        "# import re \n",
        "# class L_Model:\n",
        "# \tdef __init__(self):\n",
        "# \t\t#dowload corpus\n",
        "# \t\tnltk.download('gutenburg')\n",
        "# \t\tnltk.corpus.gutenberg.fileids()\n",
        "# \t\tcorpus = Counter()\n",
        "# \t\tfor filename in nltk.corpus.gutenberg.fileids():\n",
        "# \t\t\twords = [word.lower() for word in nltk.corpus.gutenberg.words(filename)]\n",
        "# \t\t\tcorpus.update(words)\n",
        "\t\t\n",
        "# \t\t#corpus to df\n",
        "# \t\tdf = pd.DataFrame.from_dict(corpus, orient='index').reset_index()\n",
        "# \t\tdf = df.rename(columns={'index':'word', 0:'count'})\n",
        "\n",
        "\n",
        "\t\t\n",
        "# class E_Model:\n",
        "\t\n",
        "# \tdef __init__(self,data:list[tuple[str,int]]):\n",
        "# \t\tdf = pd.DataFrame(data, columns=['error', 'count'])\n",
        "# \t\ttotal = sum(df['count'])\n",
        "# \t\tinv = 1/total\n",
        "# \t\tdf['p'] = df['count']*inv\n",
        "# \t\tself.df = df\n",
        "\t\n",
        "# \t#use regex to parse norvig's 1edit err\n",
        "# \tletterPattern = r\"[a-z|]+\"\n",
        "# \tnumberPattern = r\"\\d+\"\n",
        "\n",
        "# \tfile1 = open('count_1edit.txt', 'r')\n",
        "# \terror_list = []\n",
        "# \tcounts = []\n",
        "\t\t\n",
        "# \tfor line in file1:\n",
        "# \t\tletterMatch = re.findall(letterPattern, line)\n",
        "# \t\tnumberMatch = re.findall(numberPattern, line)\n",
        "# \t\terror = letterMatch.pop(0)\n",
        "# \t\tcount = numberMatch.pop(0)\n",
        "# \t\terror_list.append(error)\n",
        "# \t\tcounts.append(count)\n",
        "\n",
        "# class Finder:\n",
        "# \tdef __init__(self):\n",
        "\n",
        "# \t\tself.model = L_Model()\n",
        "\t\n",
        "# \t\t# if not isinstance(err,pd.DataFrame):\n",
        "# \t\t# \terr = pd.DataFrame(err)\n",
        "# \t\t# self.err = err\n",
        "\t\t\t\n",
        "# \tdef getD(self,tok:str):\n",
        "# \t\tcandidates = []\n",
        "# \t\td = 1\n",
        "# \t\tMAX = 90 #magic number set to the longest possible edit distance\n",
        "# \t\twhile not candidates and d <= MAX:\n",
        "# \t\t\tcandidates = [w for w in self.model.index if ld.distance(tok,w,d) > -1]\n",
        "# \t\t\td+=1\n",
        "# \t\treturn candidates\n",
        "\t\n",
        "# \tdef getCandidates(self,tok:str):\n",
        "# \t\tcandidates = [tok]\n",
        "# \t\tif tok not in self.model:\n",
        "# \t\t\tcandidates = self.getD(tok)\n",
        "# \t\t\tp_c =self.model[candidates]\n",
        "# \t\t\tcandidates = self.refine(p_c)\n",
        "# \t\treturn candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "VqKjpUrkOSnC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: editdistpy in c:\\python39\\lib\\site-packages (0.1.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install editdistpy\n",
        "from editdistpy import damerau_osa as ld\n",
        "\n",
        "def candidates(input, all_words):\n",
        "      # 1 edit distance away from word\n",
        "      if input in all_words: return [input]\n",
        "      d = 1\n",
        "      candidate_list = [w for w in all_words if ld.distance(input,w,d) > -1]\n",
        "      # for word in all_words:\n",
        "      #       x = min_edit_distance(input, word)\n",
        "      #       if x == 1:\n",
        "      #             candidate_list.append(word)\n",
        "      return candidate_list\n",
        "\n",
        "def spell_correct(candidate_list):\n",
        "      n=1\n",
        "      \n",
        "      probable = df.loc[candidate_list].sort_values(['P'], ascending=False).head(n)\n",
        "      # maxI = 0\n",
        "      # for i in range(len(candidate_list)):\n",
        "      #       x = P(candidate_list[i], all_words)\n",
        "      #       if x > maxX:\n",
        "      #             maxX = x\n",
        "      #             maxI = i\n",
        "      # TODO: Add the error model. Kyle, can you please make an algo that finds the first error and query the probability?\n",
        "      return list(probable.index)[0], probable['P']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def calculate_edit_type_and_edit(input_word, candidate_word):\n",
        "#     if len(input_word) == len(candidate_word):\n",
        "#         # Check for substitution\n",
        "#         diff_count = 0\n",
        "#         edit_type = \"sub\"\n",
        "#         edit = \"\"\n",
        "#         for i in range(len(input_word)):\n",
        "#             if input_word[i] != candidate_word[i]:\n",
        "#                 diff_count += 1\n",
        "#                 edit += candidate_word[i]\n",
        "#         if diff_count == 1:\n",
        "#             return edit_type, edit\n",
        "\n",
        "#     elif len(input_word) + 1 == len(candidate_word):\n",
        "#         # Check for insertion\n",
        "#         edit_type = \"ins\"\n",
        "#         edit = \"\"\n",
        "#         i, j = 0, 0\n",
        "#         while i < len(input_word) and j < len(candidate_word):\n",
        "#             if input_word[i] != candidate_word[j]:\n",
        "#                 edit += candidate_word[j]\n",
        "#                 j += 1\n",
        "#             else:\n",
        "#                 i += 1\n",
        "#                 j += 1\n",
        "#         if j == len(candidate_word):\n",
        "#             return edit_type, edit\n",
        "\n",
        "#     elif len(input_word) - 1 == len(candidate_word):\n",
        "#         # Check for deletion\n",
        "#         edit_type = \"del\"\n",
        "#         edit = \"\"\n",
        "#         i, j = 0, 0\n",
        "#         while i < len(input_word) and j < len(candidate_word):\n",
        "#             if input_word[i] != candidate_word[j]:\n",
        "#                 edit += input_word[i]\n",
        "#                 i += 1\n",
        "#             else:\n",
        "#                 i += 1\n",
        "#                 j += 1\n",
        "#         if i == len(input_word):\n",
        "#             return edit_type, edit\n",
        "\n",
        "#     # Check for transposition\n",
        "#     if len(input_word) == len(candidate_word) and input_word != candidate_word:\n",
        "#         for i in range(len(input_word) - 1):\n",
        "#             if input_word[i] == candidate_word[i + 1] and input_word[i + 1] == candidate_word[i]:\n",
        "#                 edit_type = \"trans\"\n",
        "#                 edit = input_word[i:i+2]\n",
        "#                 return edit_type, edit\n",
        "\n",
        "#     return None, None  # No valid edit type found\n",
        "\n",
        "# # Get user input\n",
        "# #user_input = input(\"Enter a word for spell correction: \")\n",
        "\n",
        "# corpus = \"This is a sample text for the spell correction model. Modeler is working fine.\"\n",
        "# model = Modeler(corpus)\n",
        "\n",
        "# print(model.model)\n",
        "\n",
        "# finder = Finder(model.model)\n",
        "\n",
        "# token = \"food\"\n",
        "# candidates = finder.getCandidates(token)\n",
        "\n",
        "# print(candidates)\n",
        "\n",
        "# results = []\n",
        "# valid_edit_types = [\"ins\", \"trans\", \"sub\" ,\"del\", ]\n",
        "# for candidate in candidates:\n",
        "#     candidate_word, probability = candidate[0], candidate[1]\n",
        "#     edit_type, edit = calculate_edit_type_and_edit(token, candidate_word)\n",
        "#     print(\"\\n\")\n",
        "#     print(edit_type,edit)\n",
        "#     print(\"\\n\")\n",
        "#     if edit_type in valid_edit_types:\n",
        "#         result = {\n",
        "#             \"word\": token,\n",
        "#             \"candidate\": candidate_word,\n",
        "#             \"edit_type\": edit_type,\n",
        "#             \"edit\": edit,\n",
        "#             \"P(c)\": probability,\n",
        "#             \"P(w|c)\": model.model[candidate_word],\n",
        "#             \"P(c) x P(w|c)\": probability * model.model[candidate_word]\n",
        "#         }\n",
        "#         results.append(result)\n",
        "#     else:\n",
        "#         result = \"broken\"\n",
        "#         results.append(result)\n",
        "\n",
        "# df = pd.DataFrame(results)\n",
        "\n",
        "# print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('mother',\n",
              " word\n",
              " mother    0.000545\n",
              " Name: P, dtype: float64)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#main function\n",
        "test_input = \"mohter\"\n",
        "c = candidates(test_input, all_words)\n",
        "spell_correct(c)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "8f4b3deeac0a5ce6c43bde11bfee6a0d7b0549337061a7646d07811ade3818cd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
